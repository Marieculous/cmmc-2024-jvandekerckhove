---
title: "Advanced Bayesian modeling"
author: "Joachim Vandekerckhove"
output:
  beamer_presentation:
    incremental: yes
header-includes:
  - \input{includes/beamerpheader.tex}
knit: (function(inputFile, encoding) {
    rmarkdown::render(inputFile, encoding = encoding, output_dir = "../pdf")
  })
---

```{r setup, include=FALSE}
# Load necessary libraries
library(ggplot2)
library(rjags)
library(coda)
library(tibble)
library(dplyr)

# Set seed for reproducibility
set.seed(123)
```

## Order of operations

  $$X  = \left[\begin{matrix}
          2 & 1 & 2\\
          1 & 1 & 5\\
    \end{matrix}\right]$$\pause

   Sum of sums: $$\sum_{r=1}^2 \left( \sum_{c=1}^3 x_{rc} \right) =
   \sum_{c=1}^3 \left( \sum_{r=1}^2 x_{rc} \right)$$\pause

   Sum of products vs.\ product of sums: $$\sum_{r=1}^2 \left( \prod_{c=1}^3 x_{rc} \right) \neq \prod_{c=1}^2 \left( \sum_{r=1}^3 x_{rc} \right)$$


## Order of operations

   In general, order of operations matters:
   $$f \circ g (x) \neq g \circ f (x)$$


## Estimating parameters is an operation

  In science, we often want to make statements about averages.\pause

  Estimating model parameters from data is an operation: $$\hat\theta = f(x)$$\pause

  Do we want the \emph{average model parameters of the data} or the \emph{model parameters of the average data}?

  $$\overline{f(x)} \neq f\left(\overline{x}\right)$$

## Insight learning

```{r fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
x <- 1:100
y <- ifelse(x < 50, 0, 1)
df1 <- data.frame(x = x, y = y)

p1 <- ggplot(df1, aes(x = x, y = y)) +
  geom_step() +
  labs(title = "One-shot learning, one participant", x = "Blocks", y = "Accuracy") +
  theme_minimal()

ggsave("includes/step1.eps", p1)
print(p1)
```


## Insight learning

```{r fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
df2 <- data.frame(x = rep(1:100, 10),
                  y = unlist(lapply(1:10, function(i) ifelse(1:100 < sample(35:65, 1), 0, 1))),
                  group = rep(1:10, each = 100))

p2 <- ggplot(df2, aes(x = x, y = y, group = group, color = as.factor(group))) +
  geom_step() +
  labs(title = "Ten participants", x = "Blocks", y = "Accuracy") +
  theme_minimal() +
  theme(legend.position = "none")

ggsave("includes/step2.eps", p2)
print(p2)
```


## Insight learning

```{r fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
curve <- function(x) {
  1 / (1 + exp(-0.1 * (x - 50)))
}
df3 <- data.frame(x = 1:100, y = curve(1:100))

p3 <- ggplot(df3, aes(x = x, y = y)) +
  geom_line() +
  labs(title = "Average of many participants", x = "Blocks", y = "Accuracy") +
  theme_minimal()
ggsave("includes/step3.eps", p3)
print(p3)
```

## Insight learning
  The ``average" learning curve looks nothing like the person-specific learning curve!\vspace{1em}

  \centering
  \includegraphics[scale=0.5]{includes/step1.eps}
  \includegraphics[scale=0.5]{includes/step3.eps}

## Insight learning
  Instead, what we want here is to acknowledge that each person has their own trajectory,
  and then say something about the (average) properties of the trajectories.\pause
  
  We want to make an abstraction of the data, which are something complicated that is 
  generated by a process with parameters $\theta_p$ (for person $p$), and instead
  focus on parameters.
  
## Basic hierarchy
  $$
  {\cal M}_h: \begin{cases}
  x_p \sim \text{one-shot}\left(\theta_p\right)\\
  \theta_p \sim N(\mu, \tau)
  \end{cases}
  $$
  
  The hierarchical model contains one set of assumptions about the data given the model (the likelihood level), and another set of assumptions about structure among parameters.\pause
  
  Here, the hierarchical parameters $\mu$ and $\tau$ tell us something about the population of participants, each with their own change point $\theta_p$.
  
## Population assumptions

An assumption made in hierarchical models is that it is possible to know things about participants merely because they are members of some sampled population.\pause

For example, just by knowing someone is a human, we can make reasonable estimates about their number of thumbs.\pause

If $n_t > 2$, a measurement error may have occurred.

## Insight learning
```{r, echo=FALSE}
n_participants <- 50
```
Back to the one-shot learning, we could try to characterize the population of participants in terms of their change point distribution $\theta_p \sim N(\mu, \tau)$.\pause

   - Let's generate learning curves for a population of `r paste0("$n = ", n_participants, "$")` participants, each with a random change point.
   - Let's use a step function to represent the learning curves.

## Insight learning
```{r simulate, echo=FALSE, fig.width = 4, fig.height = 3, fig.align='center'}
# Simulate 10 participants' learning curves
time_points <- 1:100
real_mu <- 35
real_si <-  8
thetas <- round(rnorm(n_participants, real_mu, real_si))
learning_curves <- sapply(thetas,
    function(cp) ifelse(time_points < cp, 0, 1))

# Create a data frame for plotting
learning_data <- data.frame(
  Time = rep(time_points, n_participants),
  Performance = as.vector(learning_curves),
  Participant = rep(1:n_participants, each = 100))

# Plot the simulated learning curves
p1 <- ggplot(learning_data,
             aes(x = Time,
                 y = Performance,
                 group = Participant,
                 color = as.factor(Participant))) +
  geom_step() +
  labs(title = "Simulated Learning Curves",
       x = "Time", y = "Performance") +
  theme_minimal() +
  theme(legend.position = "none")

print(p1)
```

## Hierarchical model of insight learning

   - We define a hierarchical model in JAGS to estimate each participant's change point.
   - The change point for each participant is modeled as a uniform distribution between 1 and 100.

## Hierarchical model of insight learning
```{r hierarchical, include=TRUE, echo=FALSE}
hierarchical_model <- "
model {
  for (j in 1:P) {
    for (i in 1:N) {
      y[i, j] ~ dbern(p[i, j])
      p[i, j] <- ifelse(time[i] < theta[j], 0, 1)
    }
    theta[j] ~ dnorm(mu, tau)T(0,100)
  }
  mu  ~ dnorm(50, 0.05)T(0,100)
  tau ~ dnorm(20, 0.10)T(0,)
  sigma <- pow(tau, -0.5)
}
"
cat(hierarchical_model)
```

```{r, echo=FALSE, message=FALSE, include=FALSE}
# Prepare data for JAGS
data_list_hrcl <- list(
  y    = learning_curves,
  time = time_points,
  P    = n_participants,
  N    = length(time_points)
)

# Initial values
inits_hrcl <- function() {
  list(theta = thetas)
}

# Parameters to monitor
params_hrcl <- c("mu", "sigma", "theta")

# Run the JAGS model
jags_hrcl <- jags.model(
  textConnection(hierarchical_model),
  data = data_list_hrcl,
  inits = inits_hrcl,
  n.chains = 3, n.adapt = 1000)

update(jags_hrcl, n.iter = 1000)
samples_hrcl <- coda.samples(
  jags_hrcl,
  variable.names = params_hrcl,
  n.iter = 5000)

# Extract the posterior means for the parameters
posterior_means_hrcl <- summary(samples_hrcl)$
  statistics[, "Mean"]
mu_hat_hrcl    <- posterior_means_hrcl["mu"]
sigma_hat_hrcl <- posterior_means_hrcl["sigma"]

summary(samples_hrcl)[1]$
  statistics[, c("Mean", "SD", "Time-series SE")] %>%
  print()

low  = mu_hat_hrcl - sigma_hat_hrcl
mid  = mu_hat_hrcl
high = mu_hat_hrcl + sigma_hat_hrcl

step_function_data <- data.frame(
  Time = rep(time_points, 3),
  Performance = c(ifelse(time_points <  low, 0, 1),
                  ifelse(time_points <  mid, 0, 1),
                  ifelse(time_points < high, 0, 1)),
  Function = rep(c("-1SD", "Mean", "+1SD"),
                 each = length(time_points))
)
```

## Hierarchical model of insight learning
```{r fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE}
p1 + geom_step(data = step_function_data,
            aes(x = Time, 
                y = Performance,
                color = Function,
                group = Function),
            linewidth = 1.5) +
    scale_color_manual(values = c("-1SD" = "red", "Mean" = "black", "+1SD" = "blue")) +
  labs(title = "Hierarchical model")
```

## Population results
```{r fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE}
mu_hat_hrcl    <- posterior_means_hrcl["mu"]
sigma_hat_hrcl <- posterior_means_hrcl["sigma"]
theta_hat      <- posterior_means_hrcl[grep("theta", names(posterior_means_hrcl))]

# Create a sequence of x values for the normal distribution
x_values <- seq(0, 100, length.out = 1000)

# Create a data frame for the normal distribution
normal_data <- data.frame(
  x = x_values,
  y = dnorm(x_values, mean = mu_hat_hrcl, sd = sigma_hat_hrcl)
)

# Generate random y values for the theta points under the curve
set.seed(123)  # For reproducibility
theta_y_values <- runif(length(theta_hat), min = 0, max = dnorm(theta_hat, mean = mu_hat_hrcl, sd = sigma_hat_hrcl))

theta_data <- data.frame(
  theta = theta_hat,
  y = theta_y_values
)

# Plot the normal distribution and highlight theta points with rainbow colors
ggplot(normal_data, aes(x = x, y = y)) +
  geom_line(color = "red", linewidth = 1) +  # Plot the normal distribution
  geom_point(data = theta_data, aes(x = theta, y = y, color = theta), size = 1) +  # Plot the theta points
  scale_color_gradientn(colors = rainbow(length(theta_hat))) +  # Apply the rainbow color palette
  labs(title = "Population distribution of change points",
       x = "change points",
       y = "") +
  theme_minimal() +
  theme(legend.position = "none")

```
## Parameter estimates

```{r}
summary(samples_hrcl)[1]$
  statistics[c("mu","sigma"), 
             c("Mean", "SD", "Time-series SE")] %>%
  print()
```

\pause
Compare to:

   - Simulated `r paste0("$\\mu = ", real_mu, "$")`
   - Simulated `r paste0("$\\sigma = ", real_si, "$")`


## Unorthodox things you can do if you're a Bayesian
\pause

"If you're a Bayesian you can do everything that God forbids." \newline{}( - Willem Heiser)\pause

   - Accumulate evidence for the absence of an effect
   - Draw valid conclusions with almost no data
   - Say things about means without knowing much about the basic units

## Means without knowing much about the basic units

```{r, fig.width = 4, fig.height = 3, fig.align='center', echo=TRUE, message=FALSE}
N <- 80
group <- rep(1:2, each = N/2) # Make two groups
# True score for each participant depends on group
true_score <-
  rnorm(N,
        mean = rep(c(50, 55), each = N/2),
        sd   = 2) 
# Observations are noisy
observed_score <- true_score +
  rnorm(N, mean = 0, sd = 5)

data <- data.frame(participant = 1:N,
                   group = factor(group),
                   true_score, observed_score)
```
##
```{r, fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
# Visualize the generated data
p1 <- ggplot(data, aes(x = group, y = observed_score)) +
  geom_boxplot() +
  geom_jitter(width = 0.2) +
  labs(title = "Observed Scores by Group", x = "Group", y = "Observed Score")
print(p1)
```
##
```{r}
model_string <- "
model {
  for (i in 1:N) {
    # Likelihood
    observed_score[i] ~ dnorm(true_score[i], tau)
    # Hierarchical model
    true_score[i]     ~ dnorm(mu[group[i]], tau_true)
  }
  diff_mu ~ dnorm(0, 0.001)
  mu[1]   ~ dunif(0, 100)
  mu[2] <- mu[1] + diff_mu
  tau      ~ dgamma(0.1, 0.1)
  tau_true ~ dgamma(0.1, 0.1)
}
"
```
##
```{r}
data_list <- list(
  N = N,
  observed_score = data$observed_score,
  group = as.numeric(data$group)
)

# Initial values
inits <- function() {
  list(
    tau        = rgamma(1, 0.1, 0.1),
    tau_true   = rgamma(1, 0.1, 0.1),
    true_score = runif(N, 0, 100)
  )
}
```
##
```{r}
jags_model <- jags.model(textConnection(model_string),
                         data     = data_list,
                         inits    =     inits,
                         n.chains =        3 ,
                         n.adapt  =     1000 )
```
##
\footnotesize
```{r}
update(jags_model, n.iter = 1000)
samples <- coda.samples(jags_model,
                        variable.names = c("diff_mu",
                                           "true_score"),
                        n.iter = 5000)
```

```{r}
# Summarize the results
summary_diff_mu <- summary(samples)$
  statistics["diff_mu", ]
print(summary_diff_mu)
```
##
\footnotesize
```{r}
# Extract true scores from the posterior samples
true_scores_posterior <- as.data.frame(as.matrix(samples)) %>%
  select(starts_with("true_score")) %>%
  apply(2, mean)

# Add the recovered true scores to the data frame
data$recovered_true_score <- true_scores_posterior

# Scatter plot of true scores vs recovered true scores
p2 <- ggplot(data, aes(x = true_score, y = recovered_true_score, 
                       color = group)) +
  geom_point() +
  labs(x = "True Score", y = "Recovered Score") +
  theme_minimal() +
  theme(legend.position = "none")
```
##
```{r, fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
print(p2)
```

## Hierarchical recovery beats individual recovery

The difference between groups is well recovered as $`r summary_diff_mu[[1]]` \pm `r summary_diff_mu[[2]]`$, even though the true scores within each group are poorly recovered.

```{r, fig.width = 3, fig.height = 2, fig.align='center', echo=FALSE, message=FALSE}
print(p2)
```

## Perform inference with no actual data

Suppose these were scores from a test (and suppose the two groups are an "on-track" group and an "advanced" group).  One student from the advanced group missed class. What do we know about student $N+1$?

##
```{r}
model_string <- "
model {
  for (i in 1:N) {
    # Likelihood
    observed_score[i] ~ dnorm(true_score[i], tau)
    # Hierarchical model
    true_score[i]     ~ dnorm(mu[group[i]], tau_true)
  }
  diff_mu ~ dnorm(0, 0.001)
  mu[1]   ~ dunif(0, 100)
  mu[2] <- mu[1] + diff_mu
  tau      ~ dgamma(0.1, 0.1)
  tau_true ~ dgamma(0.1, 0.1)

  true_score[N+1] ~ dnorm(mu[2], tau_true)
  observed_score[N+1] ~ dnorm(true_score[N+1], tau)
}
"
```
##
```{r}
data_list <- list(
  N = N,
  observed_score = c(data$observed_score, NA),
  group = as.numeric(data$group)
)

# Initial values
inits <- function() {
  list(
    tau        = rgamma(1, 0.1, 0.1),
    tau_true   = rgamma(1, 0.1, 0.1),
    true_score = runif(N+1, 0, 100)
  )
}
```
##
```{r}
jags_model <- jags.model(textConnection(model_string),
                         data     = data_list,
                         inits    =     inits,
                         n.chains =        3 ,
                         n.adapt  =     1000 )
```
##
\footnotesize
```{r}
update(jags_model, n.iter = 1000)
samples <- coda.samples(jags_model,
                        variable.names = c("diff_mu",
                                           "true_score",
                                           "observed_score"),
                        n.iter = 5000)
```

```{r}
# Summarize the results
summary_new <- summary(samples)$
  statistics[c("true_score[81]","observed_score[81]"), 
             c("Mean", "SD")]
print(summary_new)
```
##
\footnotesize
```{r}
# Extract true scores from the posterior samples
true_scores_posterior_new <- as.data.frame(as.matrix(samples)) %>%
  select(starts_with("true_score[81]"))

```
## True score and observed score
```{r, fig.width = 4, fig.height = 3, fig.align='center', echo=FALSE, message=FALSE}
samples_matrix <- as.matrix(samples)

true_score_samples <- samples_matrix[, "true_score[81]"]
observed_score_samples <- samples_matrix[, "observed_score[81]"]

# Create a data frame for plotting
plot_data <- data.frame(
  score = c(true_score_samples, observed_score_samples),
  type = rep(c("True Score", "Observed Score"), each = length(true_score_samples))
)

# Plot overlapping histograms with transparency
ggplot(plot_data, aes(x = score, fill = type)) +
  geom_histogram(position = "identity", alpha = 0.5, bins = 70) +  # 'alpha' controls transparency
  labs(title = "",
       x = "Score",
       y = "Frequency") +
  scale_fill_manual(values = c("True Score" = "blue", "Observed Score" = "red")) +  # Custom colors
  theme_minimal() + 
  theme(legend.position = "top")
```